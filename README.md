#基于微博文本分析的用户画像分析
##目的
由于本次作业数据的局限性，能分析的用户只有时间，微博内容。通过对文本分析的给用户打上标签。

##环境
###数据库
mongodb 3.2.x
下载安装:https://docs.mongodb.com/master/installation/
>
1.如果已经安装，可用mongod -version查看版本
2.想用gui的话可用robomongo，地址:https://robomongo.org/
3.mongodb数据导入导出教程:http://chenzhou123520.iteye.com/blog/1641319


### 开发环境
- 语言： java 1.8
- IDE： Inteilij IDEA 2016.01
- 依赖库：
   - 自然语言处理工具-HanNLP:https://github.com/hankcs/HanLP
   - mongodb driver:https://docs.mongodb.com/ecosystem/drivers/java/
   - SemanticAnalysis http://git.oschina.net/www.feit.com/SemanticAnalysis


##文件说明
- 处理代码 流程中的每一步都是单独一个可运行的模块
  - 预处理 UserMerger/*
  - 分词  DivideWord/*
  - 情感分析 EmotionAnalysis/*
  - 兴趣分析 InterestAnalysis/*
  - 时间分析 TimeAnalysis/*
  - 角色分析 RoleAnalysis/*
- 处理结果 dat/*
  - 预处理结果 weibo_active.json
  - 分词结果 weibo_keyword.json
  - 情感分析结果 weibo_emotion.json, (情感分析的中间结果：weibo_emotion_middle.json)
  - 兴趣分析结果 weibo_interest.json
  - 时间分析结果 weibo_time.json
  - 角色分析结果 weibo_role.json



##流程
### 预处理
####简单预处理
**对应模块UserMerger**
1. 原数据csv格式
源文件 data_new.csv， 数据量60W

2. 提取有效属性(id,content,time,user),存入mongodb，之后一条微博信息如下格式如下
  ```json
{
    "id" : 600357,
    "content" : "#上海微整形#你若许我不离不弃，@男神整形医生铭昊 ，我定许你地久天长。花绽流年，芭比眼，鼻综合，自体脂肪填[微笑]充，情事摇曳，今生我愿为你舒展广袖，假体隆胸隆鼻留幽香于袖底，在楼榭上，水岸边，清幽处，倾尽舞步，舞尽岁月娉婷，舞尽流年芳菲。",
    "time" : "2016/3/16 17:45",
    "user" : "qetina"
}
  ```

3. 坏数据清洗
删除无名数据，如现在数据内部有2567条用户名是"#NAME？"，也就是无名用户的删除

4. 相同用户整合
统计出用户的微博集和活动时间集，样例如下
  ```json
{
    "user" : "niglBBy",
    "contents" : [
        "胡歌上海旅游形象大使。四季上海，天天精彩。 .....",
        "谁粉我我立马粉水！ 【原微博】 @Alina童颜-微整形 不要为明天忧虑，因为明天自有明天的忧虑；一天的难处担当就够了。而我们大人整天都强迫孩子去做一些事情。#上海微整形#却从来没有问过他们快乐不快乐，原来真正蒙在鼓里的是我们",
        "来自星星的微博[心] 【原微博】 @Alina童颜-微整形 不要为明天忧虑，因为明天自有明天的忧虑；一天的难处担当就够了。而我们大人整天都强迫孩子去做一些事情。#上海微整形#却从来没有问过他们快乐不快乐，原来真正蒙在鼓里的是我们",
        "胡歌上海旅游形象大使。四季上海，天天精彩。 【原微博】 #晒春游逛上海#樱花、桃花、郁金香、油菜花，看不尽的花；公园、马路、古镇，逛不完的景；青团、烧卖、春笋、刀鱼面，吃不完的美食……还不跟我一起去春游？@乐游上海"
    ],
    "times" : [
        "2016/3/22 10:55",
        "2016/3/22 10:53",
        "2016/3/22 10:52",
        "2016/3/22 10:52"
    ],
    "count" : 4
}
  ```

5. 活跃用户筛选
因为是做用户画像，那些发表数量少的用户无法准确添加标签，所以筛选了在该数据时间段内的发表微博数在10以上的用户，从204401名用户中筛选了共9950名用户。
到这一步完成的数据在百度云链接: http://pan.baidu.com/s/1miCORJ6 密码: gzva
**一共9950条数据，如果出现数据爆内存了（自动断了链接），请在import语句最后加上 --batchSize 1,就能导入成功了**

####文本分词
提取微博内容中的名词和实体，然后形容词
如果可以的话，要得到每个实体的比重

### 基于分析的处理

#### 自然属性
目前从数据上来看，只有时间和发的微博内容，无法分析

#### 性格分析
根据形容用词和性格用词的近似度去匹配。领域用词需要自己去调。

#### 兴趣/话题分析
#####根据
原微博的分词形成的关键词和自带话题。
根据原微博的关键词，利用语义近似度（尝试了很多语料库，最后采用《同义词词林扩展版》），
试验了两种方法，根据原有主题生成单词云
#####尝试
先尝试利用词向量与语义距离寻找话题，就是根据名词和几个预设领域名词的近似度去匹配（就是把一个词用几个词向量来表示），找寻和一篇文章中大部分关键词距离之和最近的那个领域。然后我也试了几个主流的中文自然语言领域（比如波森nlp
和腾讯文智，复旦那个nlp）的分析处理，发现他们的这套算法基本也是非常不准的，这是一个理论上分析话题的很好的方法，但是实际效果十分受限于语料关系。比如我们现在采用的比较通用的语料是《同义词词林扩展版》，对人名、书名的语料信息十分不全。
#####方法
预设主题的方法失效了，最后我们采用对关键词和微博自带话题做名词实体的提取，根据不同的名词类型（机构、人名、地名、普通名词等）分类，找到每个名词类型下占有极大权重的名词，小权重就直接忽略。比如对于微博话题“上海微整形”和“上海北京西安榆林南昌长沙宝鸡咸阳微整形”的结果分别是【上海，微整形】和【微整形】。
处理之后两个例子如下：
```
{
    "user" : "-希帕蒂娅-",
    "interest" : [ "微整形", "王思聪", "美容", "整形" ]
}
{
    "user":"wj悲凉仰妇胃幸",
    "interest":["颜控","吃货","田子坊","胡歌","花海","旅游","上海"]
}
```



#### 时间作息分析
根据每个人时间段的密度

#### 角色分析
区分转发狂魔和发布者，依靠数量统计,


### 准确度

### 群体性行为和个体性行为差异

### 应用
因为这次数据大多数微博的内容十分单一，在话题分析的出结果是不有趣的，并不具有十分大的利用价值。

## 参考链接
如何构建用户画像- 概述 - http://www.woshipm.com/pmd/107919.html
新手如何开始用户画像分析 - https://www.zhihu.com/question/29468464
可视化工具推荐 - https://www.zhihu.com/question/31429786
word2vec - http://wei-li.cnblogs.com/p/word2vec.html